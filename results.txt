Joseph Del Rocco
04/20/2008


---------------------------------------
a. MATRIX MULTIPLY
---------------------------------------

matrix_spu.c        - 77  statements
matrix_spu.assembly - 223 instructions

Matrix B is transposed in the PPU to support reading rows instead
of columns.  Each SPE works on certain rows of the solution matrix
(SPE 0 works on rows 0,6,12,18 and SPE 1 works on 1,7,13,19 etc).
Below is the algorithm for each SPE.
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
For each row to work on
  dma equivalent row from input matrix A
  For each "column" in matrix B (actually row because its transposed)
    dma "column"
    sum up row x column
    set approprite element in resulting row
  dma resulting row back to output matrix PPU
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
Double buffering is done between the column reads to overlap them.  And
I don't block on writing the result row back until the next write.
Known issues are lack of vectorization, and since I read entire rows at
a time the max size matrix supported is 4096x4096 since 4096x4b = 16K.

Matrix:   4096x4096
PPU Time: ----------
SPU Time: 90.260000s
Gigflops: 1.41813

Matrix:   2048x2048
PPU Time: ----------
SPU Time: 10.810000s
Gigflops: 1.48011

Matrix:   1024x1024
PPU Time: 148.090000s
SPU Time: 1.350000s
Gigflops: 1.48148

Matrix:   512x512
PPU Time: 17.940000s
SPU Time: 0.170000s
Gigflops: 1.47059

Matrix:   256x256
PPU Time: 0.410000s
SPU Time: 0.020000s
Gigflops: 1.56250


---------------------------------------
b. IMAGE CONVOLUTION
---------------------------------------

convolution_spu.c        - 124 statements
convolution_spu.assembly - 445 instructions

First of all, I flipped the algorithm to:
 h[j,k]*a[m+j,n+k]  instead of  h[j,k]*a[m-j,n-k]
Thus the values convolve to the top-left instead of the bottom-right.
This makes it easier to simply pad the matrix on the right and bottom
with zeros, instead of padding on the top and left sides, and you
don't have to worry about an offset into the image when reading.
Essentially the algorithm is the same, just a little easier to program.
Each SPE only works on certain rows of tiles of the resulting image
(SPE 0 works on tile row 0,6,12,18 and SPE 1 works on 1,7,13,19 etc).
Below is the algorithm for each SPE.
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
First dma entire filter(h) into local store
For each tile row in image to work on
  For each tile in that row
    dma a "block" from input matrix A (tile+pad to account for h overlap)
    do convolution algorithm, set results in a local store tile
    dma resutling tile back to PPU
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
Double buffering is done between tile reads from image A.  And I don't
block on writing resulting tiles back to image C, until the next write.
Known issues are lack of vectorization, and it only supports filter
size of power of two - thus 2x2, 4x4, 8x8 and 16x16.

Matrix:   4096x4096
Filter:   8x8
PPU Time: 12.060000s
SPU Time: 3.670000s
Gigflops: 0.54496

Matrix:   2048x2048
Filter:   8x8
PPU Time: 1.300000s
SPU Time: 0.520000s
Gigflops: 0.96154

Matrix:   1024x1024
Filter:   8x8
PPU Time: 0.240000s
SPU Time: 0.140000s
Gigflops: 0.89286

Matrix:   512x512
Filter:   8x8
PPU Time: 0.060000s
SPU Time: 0.030000s
Gigflops: 1.04167

Matrix:   256x256
Filter:   8x8
PPU Time: 0.020000s
SPU Time: 0.010000s
Gigflops: 0.78125

--
Matrix:   4096x4096
Filter:   4x4
PPU Time: 7.050000s
SPU Time: 1.350000s
Gigflops: 0.37037

Matrix:   2048x2048
Filter:   4x4
PPU Time: 0.260000s
SPU Time: 0.150000s
Gigflops: 0.83333

Matrix:   1024x1024
Filter:   4x4
PPU Time: 0.060000s
SPU Time: 0.040000s
Gigflops: 0.78125

Matrix:   512x512
Filter:   4x4
PPU Time: 0.010000s
SPU Time: 0.010000s
Gigflops: 0.78125

Matrix:   256x256
Filter:   4x4
PPU Time: 0.010000s
SPU Time: 0.000000s
Gigflops: ---------
